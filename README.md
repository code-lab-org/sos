# SOS

This repository contains the codebase for Snow Observing Strategy (SOS) applications integrated within the [Novel Observing Strategies Testbed (NOS-T)](https://github.com/code-lab-org/nost-tools).

- [SOS](#sos)
  - [Installation](#installation)
    - [NOS-T Tools Installation](#nos-t-tools-installation)
    - [AWS CLI Installation](#aws-cli-installation)
      - [Install AWS CLI](#install-aws-cli)
        - [Linux](#linux)
        - [Windows](#windows)
        - [Mac](#mac)
      - [Configure AWS CLI](#configure-aws-cli)
  - [Introduction](#introduction)
  - [Execution](#execution)
    - [Docker (Development)](#docker-development)
    - [Docker Compose](#docker-compose)


## Installation

### NOS-T Tools Installation

To install the NOS-T library, follow the directions [here](https://nost-tools-v2.readthedocs.io/en/latest/installation/installation.html).

### AWS CLI Installation

The SOS applications use the Amazon Web Services (AWS) command line interface (CLI).

#### Install AWS CLI

Installation instructions are provided below. For further information on AWS CLI installation, [click here](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).

##### Linux

```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

##### Windows

1. Download and run the AWSL CLI installer: 

```powershell
msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi
```

2. Confirm successful installation

```powershell
aws --version
```

##### Mac

1. Download AWS CLI installer:

```bash
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
```

2. Run the installer: 

```bash
sudo installer -pkg ./AWSCLIV2.pkg -target /
```

3. Confirm successful installation:

```bash
aws --version
```

#### Configure AWS CLI

Once installed, the AWS CLI must be configured:

```bash
aws configure
```

Enter the Access Key ID and Secret Access Key provided by the NOS-T operator.

## Introduction

A single manager application is responsible for orchestrating the various applications and keeping a consistent time across applications. Upon initiation of the manager, various managed applications are triggered, each responsible for generating derived, merged datasets or raster layers sent as base64-encoded strings. Below is a table describing each application:

|Application|Purpose|Data Source|Developed|Containerized|
|:---------:|:-----:|:---------:|:-------:|:-----------:|
|Manager|Orchestrates applications, maintains time|NA|Y|Y|
|Planner|Selects best taskable observations on the basis of reward|LIS|Y|N|
|Appender|Aggregates planned taskable observations, filtering duplicates|Planner|Y|N|
|Simulator|Simulates satellite operations and determines when and where observations are collected|Appender|Y|N|


Applications communicate via a RabbitMQ message broker utilizing the Advanced Message Queuing Protocol (AMQP) protocol. The figure below illustrates the overall workflow:

<p align="center">
  <img src="https://pointillism.io/code-lab-org/sos/3-update-documentation-to-reflect-current-code/docs/workflow.dot.svg"/>
  <br>
  <em>Snow Observing Systems (SOS) application workflow.</em>
</p>

The input data and output data generated by applications are uploaded onto an Amazon Web Services (AWS) Simple Storage Service (S3) bucket with the following data structure:

```bash
snow_observing_systems/
└── daily
    ├── 2019-03-01
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-02
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-03
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-04
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-05
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-06
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-07
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-08
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    ├── 2019-03-09
    │   ├── Appender
    │   ├── Planner
    │   └── Simulator
    └── 2019-03-10
        ├── Appender
        ├── Planner
        └── Simulator
```

The applications use the AWS SDK for Python, [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html). Boto3 allows users to create, configure, and manage AWS services, including S3, Simple Notification Service (SNS), and Elastic Compute Cloud (EC2). Access to the AWS SDK is limited to SOS administrators as required by NASA's [Science Managed Cloud Environment (SMCE)](https://smce.nasa.gov/). 

<p align="center">
  <img src="https://pointillism.io/code-lab-org/sos/3-update-documentation-to-reflect-current-code/docs/aws.dot.svg"/>
  <br>
  <em>Amazon Web Services (AWS) resources used by the SOS applications within the NOS-T system include S3 and Lambda.</em>
</p>

## Execution

### Docker (Development)

Each container can be built individually during development, to build a local version of a container, you can use ```docker build```. 

For example, to build the "manager" comntainer:

```
cd src/manager/
docker build -t sos_manager .
docker run --rm --env-file .env -v "$(pwd)/data":/opt/data sos_manager
```

> NOTE: You can follow similar steps to build the other containers, such as satellites, layer_resolution, layer_snow_cover, etc.

### Docker Compose
Three applications, including manager, satellite, and snow cover layer applications, can be run using Docker compose. To run applications, do the following:

1. Download input data, which should be in the following structure:

    ```
    data/
    ├── Downloaded_files
    │   ├── Mo_basin_shp
    │   │   ├── WBD_10_HU2_Shape
    │   │   └── WBD_10_HU2_Shape.zip
    │   ├── Resolution_raw
    │   │   └── SNODAS
    │   └── Snow_cover_raw
    │       ├── hdf
    │       └── nc
    ├── Efficiency_files
    │   ├── Efficiency_high_resolution_Caesium
    │   │   ├── efficiency_resolution.nc
    │   │   ├── efficiency_resolution_taskable.nc
    │   │   ├── efficiency_snow_cover.nc
    │   │   └── efficiency_snow_cover_up.nc
    │   └── Efficiency_resolution20_Optimization
    │       ├── Efficiency_SWE_Change_dataset_Capella.nc
    │       ├── Efficiency_Sensor_dataset.nc
    │       ├── Efficiency_Sensor_dataset_Capella.nc
    │       ├── Efficiency_Sensor_dataset_GCOM.nc
    │       ├── Efficiency_Temperature_dataset.nc
    │       ├── Efficiency_Temperature_dataset_coarsened.nc
    │       ├── Optimization_result.geojson
    │       ├── Temperature_dataset.nc
    │       ├── coarsened_eta_output_Capella.nc
    │       ├── coarsened_eta_output_GCOM.nc
    │       ├── efficiency_resolution_layer.nc
    │       ├── efficiency_snow_cover.nc
    │       ├── eta0_resampled_to_match_coarsened_grid.nc
    │       ├── final_blocks_rewards.geojson
    │       ├── final_eta_combined_output_Capella.nc
    │       └── final_eta_combined_output_GCOM.nc
    └── Preprocessed_files
        ├── preprocessed_resolution.nc
        └── preprocessed_snow_cover.nc
    ```

1. Confirm you have a .env file in your working directory with the following contents:

    ```
    # FILE/DIR PATHS 
    path_hdf=data/Downloaded_files/Snow_cover_raw/hdf
    path_nc=data/Downloaded_files/Snow_cover_raw/nc/
    path_shp=data/Downloaded_files/Mo_basin_shp/
    path_preprocessed=data/Preprocessed_files/
    path_efficiency=data/Efficiency_files/Efficiency_high_resolution_Caesium/
    raw_path=data/Downloaded_files/Resolution_raw/

    # EARTHACCESS LOGIN
    EARTHDATA_USERNAME=<Your EarthAccess username>
    EARTHDATA_PASSWORD=<Your EarthAccess password>

    # NOS-T LOGIN
    HOST=<Contact NOS-T admins>
    KEYCLOAK_PORT=8443
    KEYCLOAK_REALM=<Contact NOS-T admins>
    RABBITMQ_PORT=5671
    USERNAME=<Your Keycloak username in NOS-T ecosystem>
    PASSWORD=<Your Keycloak password in NOS-T ecosystem>
    CLIENT_ID=<Contact NOS-T admins>
    CLIENT_SECRET_KEY=<Contact NOS-T admins>
    VIRTUAL_HOST="/"
    IS_TLS=True

    # CESIUM LOGIN
    TOKEN=<Cesium access token>
    ```

1. Orchestrate the containers:

    ```
    docker-compose up -d
    ```

    > NOTE: To confirm Docker containers are running, run the command: ```docker ps```. You should see three containers list: sos_manager, sos_satellites, and sos_snow_cover_layer.

1. To shutdown the Docker containers:

    ```
    docker-compose down
    ```